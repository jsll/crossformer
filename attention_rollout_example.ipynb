{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534daf7f-4b6b-4357-9a38-9117f72ce9b4",
   "metadata": {},
   "source": [
    "# Step 1: Minimal CrossFormer Inference Example\n",
    "\n",
    "This Colab demonstrates how to load a pre-trained / finetuned CrossFormer checkpoint, run inference for a single-arm and bimanual manipulation system, and compare the outputs to the true actions.\n",
    "\n",
    "First, let's start with a minimal example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5312c8-fa0e-4f0e-91a6-0c4e82153f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import datasets\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import io\n",
    "from crossformer.utils import visualization_utils\n",
    "\n",
    "#import mediapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from crossformer.model.crossformer_model import CrossFormerModel\n",
    "\n",
    "model = CrossFormerModel.load_pretrained(\"hf://rail-berkeley/crossformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f310e-8012-4d2f-9e0e-154834296035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['CURL_CA_BUNDLE'] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4b836-80ca-4210-b82c-b1b9a4de7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(\n",
    "    builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\"\n",
    ")\n",
    "ds = builder.as_dataset(split=\"train[:1]\")\n",
    "\n",
    "# sample episode and resize to 224x224 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode[\"steps\"])\n",
    "images = [\n",
    "    cv2.resize(np.array(step[\"observation\"][\"image\"]), (224, 224)) for step in steps\n",
    "]\n",
    "\n",
    "# extract goal image and language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = (\n",
    "    steps[0][\"observation\"][\"natural_language_instruction\"].numpy().decode()\n",
    ")\n",
    "\n",
    "# visualize episode\n",
    "print(f\"Instruction: {language_instruction}\")\n",
    "#mediapy.show_video(images, fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd4b87-7609-499f-9c46-44117beb93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Instruction: {language_instruction}\")\n",
    "print(goal_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f6034-d32f-4808-b121-5d2274ead6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenX-Embodiment dataset\n",
    "dataset_names = [\"kuka\", \"bc_z\", \"cmu_playing_with_food\"]\n",
    "#loaded_datasets = []\n",
    "data_per_dataset = {}\n",
    "for dataset_name in dataset_names:\n",
    "    ds = datasets.load_dataset(\"jxu124/OpenX-Embodiment\", dataset_name, streaming=True, split='train', trust_remote_code=True)  # IterDataset\n",
    "    #loaded_datasets.append(ds)\n",
    "    random_item = next(itertools.islice(ds, 10, 10 + 1))\n",
    "    data_per_dataset[dataset_name] = random_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565e6f5-c5dd-442b-b83e-5548b50498c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_observation(data, WINDOW_SIZE=1):\n",
    "    img = Image.open(io.BytesIO(data['data.pickle']['steps'][0]['observation']['image']['bytes']))\n",
    "    img = img.resize((224,224))\n",
    "    #display(img)\n",
    "    img = np.asarray(img)\n",
    "    input_images = np.stack(img)[None,None]\n",
    "    observation = {\n",
    "        \"image_primary\": input_images,\n",
    "        \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "    }\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02739169-ff0f-4352-80ac-0a11353ecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_task(instruction, model):\n",
    "    #print(data['data.pickle']['steps'][0].keys())\n",
    "    #language_instruction = data['data.pickle']['steps'][0]['language_instruction']\n",
    "    task = model.create_tasks(texts=[instruction])  # for language conditioned\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b5038-39a2-4b07-b01b-e9caf883ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(visualization_utils)\n",
    "language_instruction = \"Locomote to the green box\"\n",
    "for data in data_per_dataset.keys():\n",
    "    observation = create_observation(data_per_dataset[data])\n",
    "    task = create_language_task(language_instruction, model)\n",
    "    rollout, token_types = model.analyze_attention(observation, task)\n",
    "    fig = visualization_utils.plot_readout_attention(rollout, token_types, \"readout_single_arm\", observation, observation_type=\"_primary\", observation_image=observation[\"image_primary\"][0,0], title=language_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87753f1f-ce7f-45ba-943f-4032b0fd0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(visualization_utils)\n",
    "language_instruction = \"Pick up the white brush\"\n",
    "for data in data_per_dataset.keys():\n",
    "    observation = create_observation(data_per_dataset[data])\n",
    "    task = create_language_task(language_instruction, model)\n",
    "    rollout, token_types = model.analyze_attention(observation, task)\n",
    "    fig = visualization_utils.plot_readout_attention(rollout, token_types, \"readout_single_arm\", observation, observation_type=\"_primary\", observation_image=observation[\"image_primary\"][0,0], title=language_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaf07b-88fa-4818-990d-31dc366a44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "\n",
    "# create task dictionary\n",
    "task = model.create_tasks(\n",
    "    goals={\"image_primary\": goal_image[None]}\n",
    ")  # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])  # for language conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e7ad4-a7a4-460a-a09e-722f03cefec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "input_images = np.stack(images[0 : 0 + WINDOW_SIZE])[None]\n",
    "observation = {\n",
    "    \"image_primary\": input_images,\n",
    "    \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d8714-f4d6-49e6-9c8a-13c49863e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout, token_types = model.analyze_attention(observation, task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4ba89-6977-4b7f-95d6-76a1d2c20b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossformer.utils import visualization_utils\n",
    "#import importlib\n",
    "#importlib.reload(visualization_utils)\n",
    "fig = visualization_utils.plot_readout_attention(rollout, token_types, \"readout_nav\", observation, observation_type=\"_primary\", observation_image=observation[\"image_primary\"][0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
