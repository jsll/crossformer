{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534daf7f-4b6b-4357-9a38-9117f72ce9b4",
   "metadata": {},
   "source": [
    "# Step 1: Minimal CrossFormer Inference Example\n",
    "\n",
    "This Colab demonstrates how to load a pre-trained / finetuned CrossFormer checkpoint, run inference for a single-arm and bimanual manipulation system, and compare the outputs to the true actions.\n",
    "\n",
    "First, let's start with a minimal example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5312c8-fa0e-4f0e-91a6-0c4e82153f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from crossformer.model.crossformer_model import CrossFormerModel\n",
    "\n",
    "model = CrossFormerModel.load_pretrained(\"hf://rail-berkeley/crossformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c063f-a07c-434f-82aa-ca4bab34cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def analyze_attention(model, observation, task):\n",
    "    \"\"\"\n",
    "    Computes attention rollout from readout tokens to input tokens.\n",
    "    Returns the attention rollout weights and a mapping from token indices to token names.\n",
    "    \n",
    "    Args:\n",
    "        observations: Dictionary of observations \n",
    "        tasks: Dictionary of task specifications\n",
    "        head_name: Name of the readout head to analyze\n",
    "    Returns:\n",
    "        rollout: Attention rollout weights from readout to input tokens\n",
    "        token_map: Mapping from token indices to token types\n",
    "    \"\"\"\n",
    "    # Run transformer with attention weights stored\n",
    "    transformer_outputs = model.module.apply(\n",
    "        {\"params\": model.params},\n",
    "        observation,\n",
    "        task,\n",
    "        observation[\"timestep_pad_mask\"],\n",
    "        train=False,\n",
    "        method=\"crossformer_transformer\",\n",
    "        mutable=[\"intermediates\"],\n",
    "    )\n",
    "\n",
    "    outputs, variables = transformer_outputs\n",
    "\n",
    "        # Count and print token breakdown\n",
    "    token_counts = {}\n",
    "\n",
    "    print(outputs.keys())\n",
    "    # Count prefix tokens\n",
    "    for prefix_group in outputs.keys():\n",
    "        if prefix_group.startswith(\"task_\"):\n",
    "            n_tokens = outputs[prefix_group].tokens.shape[-2]\n",
    "            token_counts[prefix_group] = n_tokens\n",
    "            \n",
    "    # Count observation tokens\n",
    "    for obs_group in outputs.keys():\n",
    "        if obs_group.startswith(\"obs_\"):\n",
    "            n_tokens = outputs[obs_group].tokens.shape[-2]\n",
    "            token_counts[obs_group] = n_tokens\n",
    "            \n",
    "    # Count readout tokens\n",
    "    head_names = ['readout_bimanual', 'readout_nav', 'readout_quadruped', 'readout_single_arm']\n",
    "    for readout_key in head_names:\n",
    "        if readout_key in outputs:\n",
    "            n_tokens = outputs[readout_key].tokens.shape[-2]\n",
    "            token_counts[readout_key] = n_tokens\n",
    "\n",
    "    \n",
    "    attention_weights = []\n",
    "\n",
    "\n",
    "    \n",
    "    # Extract attention weights from each transformer block\n",
    "    for i in range(model.config[\"model\"][\"transformer_kwargs\"][\"num_layers\"]):\n",
    "        block_name = f'encoderblock_{i}'\n",
    "        if block_name in variables['intermediates']['crossformer_transformer']['BlockTransformer_0']['Transformer_0']:\n",
    "            layer_attention = variables['intermediates']['crossformer_transformer']['BlockTransformer_0']['Transformer_0'][block_name]['MultiHeadDotProductAttention_0']['attention_weights'][0]\n",
    "            attention_weights.append(layer_attention)\n",
    "\n",
    "    print(\"Token count breakdown:\")\n",
    "    total = 0\n",
    "    for k, v in token_counts.items():\n",
    "        print(f\"{k}: {v} tokens\")\n",
    "        total += v\n",
    "    print(f\"Total tokens: {total}\")\n",
    "\n",
    "    \n",
    "    # Average attention weights across heads\n",
    "    attention_weights = [jnp.mean(weights, axis=1) for weights in attention_weights]\n",
    "    \n",
    "    # Build token type list\n",
    "    token_types = []\n",
    "    \n",
    "    # Map prefix tokens\n",
    "    for prefix_group in outputs.keys():\n",
    "        print(prefix_group)\n",
    "        if \"_\" in prefix_group:\n",
    "            n_tokens = outputs[prefix_group].tokens.shape[-2]\n",
    "            token_types.extend([prefix_group] * n_tokens)\n",
    "            \n",
    "    # # Map observation tokens\n",
    "    # for obs_group in outputs.keys():\n",
    "    #     if obs_group.startswith(\"obs_\"):\n",
    "    #         n_tokens = outputs[obs_group].tokens.shape[-2]\n",
    "    #         token_types.extend([obs_group] * n_tokens)\n",
    "\n",
    "    # # Map readout tokens\n",
    "    # readout_key = f\"readout_{head_name}\"\n",
    "    # if readout_key in outputs:\n",
    "    #     n_tokens = outputs[readout_key].tokens.shape[-2]\n",
    "    #     token_types.extend([readout_key] * n_tokens)\n",
    "\n",
    "    # Compute attention rollout\n",
    "    rollout = attention_weights[0]\n",
    "    for attention in attention_weights[1:]:\n",
    "        rollout = jnp.matmul(attention, rollout)\n",
    "    \n",
    "    # Normalize rollout\n",
    "    rollout = rollout / rollout.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    return rollout, token_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a0454-703a-4a18-a1eb-1614552a584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def plot_attention_rollout(\n",
    "    observations: dict, \n",
    "    rollout: np.ndarray,\n",
    "    token_types: list,\n",
    "    save_path: str = None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualizes attention rollout from readout tokens to input tokens.\n",
    "    \n",
    "    Args:\n",
    "        observations: Dictionary of observations\n",
    "        rollout: Attention rollout matrix [num_timesteps, num_tokens]\n",
    "        token_types: List of token type names\n",
    "        save_path: Optional path to save visualization\n",
    "    Returns:\n",
    "        matplotlib Figure\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    n_timesteps = observations[\"timestep_pad_mask\"].shape[1]\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for t in range(n_timesteps):\n",
    "        plt.subplot(1, n_timesteps, t+1)\n",
    "        \n",
    "        # Get image tokens for this timestep\n",
    "        obs_token_idxs = [i for i, name in enumerate(token_types) if name.startswith(\"obs_\")]\n",
    "        \n",
    "        # Get observation attention\n",
    "        obs_attention = rollout[t, obs_token_idxs]\n",
    "        \n",
    "        # Get image for this timestep\n",
    "        for k,v in observations.items():\n",
    "            if k.startswith(\"image_\"):\n",
    "                img = v[0,t]\n",
    "                break\n",
    "        \n",
    "        # Plot image\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        # Calculate grid size for reshaping attention\n",
    "        # The number of patches should be (H/patch_size) * (W/patch_size)\n",
    "        H, W = img.shape[:2]\n",
    "        patch_size = 32  # This is typically 16 or 32 for ViT\n",
    "        num_patches_h = H // patch_size\n",
    "        num_patches_w = W // patch_size\n",
    "        \n",
    "        # Reshape attention to match image patches\n",
    "        try:\n",
    "            attention_grid = obs_attention[:num_patches_h * num_patches_w].reshape(num_patches_h, num_patches_w)\n",
    "        except ValueError:\n",
    "            # If reshape fails, use interpolation to resize attention to match grid\n",
    "            attention_grid = obs_attention.reshape(1, -1)\n",
    "            attention_grid = jax.image.resize(attention_grid, (num_patches_h, num_patches_w), method='bilinear')\n",
    "            \n",
    "        # Resize attention grid to match image size\n",
    "        attention_resized = jax.image.resize(attention_grid, img.shape[:2], method='bilinear')\n",
    "        \n",
    "        # Plot attention overlay\n",
    "        plt.imshow(attention_resized, cmap='hot', alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Timestep {t}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_readout_attention(\n",
    "    observations: dict,\n",
    "    rollout: np.ndarray,\n",
    "    token_types: list,\n",
    "    save_path: str = None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        observations: Dictionary of observations\n",
    "        rollout: Attention rollout matrix\n",
    "        token_types: List of token type names\n",
    "        save_path: Optional path to save visualization\n",
    "    Returns:\n",
    "        matplotlib Figure\n",
    "    \"\"\"\n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(rollout, cmap='viridis')\n",
    "    \n",
    "    # Add token labels\n",
    "    # Only show a subset of ticks to avoid overcrowding\n",
    "    n_tokens = len(token_types)\n",
    "    step = max(1, n_tokens // 20)  # Show at most 20 ticks\n",
    "    \n",
    "    ax.set_xticks(np.arange(0, n_tokens, step))\n",
    "    ax.set_yticks(np.arange(0, n_tokens, step))\n",
    "    ax.set_xticklabels([token_types[i] for i in range(0, n_tokens, step)], rotation=45, ha='right')\n",
    "    ax.set_yticklabels([token_types[i] for i in range(0, n_tokens, step)])\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im)\n",
    "    plt.title('Attention Rollout')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05934a4-ff4c-4aac-8d85-d8e700568d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll demonstrate how to create an observation and task dictionary for a bimanual task. \n",
    "# Then we'll use them to sample an action from the model.\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "# create a random image\n",
    "img = np.random.randint(0, 255, size=(224, 224, 3))\n",
    "# add batch and observation history dimension (CrossFormer accepts a history of up to 5 time-steps)\n",
    "img = img[None, None]\n",
    "# our bimanual training data has an overhead view and two wrist views\n",
    "observation = {\n",
    "    \"image_high\": img,\n",
    "    \"image_left_wrist\": img,\n",
    "    \"image_right_wrist\": img,\n",
    "    \"timestep_pad_mask\": np.array([[True]]),\n",
    "}\n",
    "# create a task dictionary for a language task\n",
    "task = model.create_tasks(texts=[\"uncap the pen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b060a2-a640-4264-9bf9-e5af5cf28ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout, token_types = analyze_attention(model, observation, task)\n",
    "print(rollout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80644c-8ec9-4eee-a50e-3929dcae5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(token_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f233863-206f-46a0-8e16-944fa37a8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_image(observation, observation_type):\n",
    "    image=None\n",
    "    for k in observation.keys():\n",
    "        if observation_type in k:\n",
    "            image = observation[k].squeeze()\n",
    "            break\n",
    "    return image\n",
    "\n",
    "def plot_readout_attention2(\n",
    "    rollouts,\n",
    "    token_types,\n",
    "    head,\n",
    "    observations,\n",
    "    observation_type = \"_high\",\n",
    "    save_path: str = None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots attention weights from readout tokens as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        model: CrossFormerModel instance  \n",
    "        observations: Dictionary of observations\n",
    "        tasks: Dictionary of task specifications\n",
    "        readout_name: Name of readout head to analyze\n",
    "        save_path: Optional path to save visualization\n",
    "    Returns:\n",
    "        matplotlib Figure\n",
    "    \"\"\"\n",
    "    indexes_readout = []\n",
    "    indexes_obs = []\n",
    "    for i, j in enumerate(token_types):\n",
    "        if j == head:\n",
    "            indexes_readout.append(i)\n",
    "        if observation_type in j:\n",
    "            indexes_obs.append(i)\n",
    "\n",
    "    num_timesteps =  rollouts.shape[0]\n",
    "    num_images = len(indexes_readout)\n",
    "    # Create a grid of subplots: num_images rows Ã— num_timesteps columns\n",
    "    fig, axs = plt.subplots(1, num_timesteps+1,squeeze=False) \n",
    "                           #figsize=(4*num_timesteps, 4*num_images),squeeze=False)\n",
    "\n",
    "    observation_image = get_observation_image(observations, observation_type)\n",
    "    im = axs[0, 0].imshow(observation_image, cmap='viridis')\n",
    "\n",
    "    # If there's only one image, wrap axs in a list to make it 2D\n",
    "    if num_images == 1:\n",
    "        axs = np.array([axs])\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        rollout = rollouts[t]\n",
    "        # Create heatmap\n",
    "    \n",
    "        images_per_readout = []\n",
    "        patch = np.zeros((32,32))\n",
    "        for num_image in range(num_images):\n",
    "            image = np.zeros((224,224))\n",
    "            patches = []\n",
    "            x=0\n",
    "            y=0\n",
    "            for index_obs in indexes_obs:\n",
    "                attention = rollout[indexes_readout[num_image], index_obs]\n",
    "                patch[:,:] = attention\n",
    "                image[x:x+32, y:y+32] = attention\n",
    "                x+= 32\n",
    "                if x== 224:\n",
    "                    x=0\n",
    "                    y+=32\n",
    "            images_per_readout.append(image.copy())\n",
    "            # Plot the image in the corresponding subplot\n",
    "        average_readout_image = np.asarray(images_per_readout).mean(0)\n",
    "        # Plot original image\n",
    "        axs[0, t+1].imshow(observation_image)\n",
    "        # Overlay attention map with alpha\n",
    "        # Convert attention to alpha values (darker where attention is higher)\n",
    "        alpha = average_readout_image / average_readout_image.max()  # Normalize to [0,1]\n",
    "        alpha = 1 - alpha  # Invert so high attention = darker\n",
    "        \n",
    "        # Create a dark overlay\n",
    "        dark_overlay = np.zeros_like(observation_image)\n",
    "        axs[0, t+1].imshow(dark_overlay, alpha=alpha, cmap='gray')\n",
    "\n",
    "        axs[0,t+1].axis('off')  # Remove axes\n",
    "            \n",
    "        axs[0, t+1].set_title(f'Timestep {t}')\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "    # Add a colorbar that applies to all subplots\n",
    "    #fig.colorbar(im, ax=axs.ravel().tolist())\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17bcf0-fb9a-4582-9ae4-976085613d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_readout_attention2(rollout, token_types, \"readout_single_arm\", observation)\n",
    "#plot_attention_rollout(observation, rollout, token_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ad189-0402-4dae-a6fa-3ff3dce28db2",
   "metadata": {},
   "source": [
    "# Step 2: Run Inference on Full Trajectories\n",
    "\n",
    "That was easy! Now let's try to run inference across a whole single-arm trajectory and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f7fd1-5b43-480f-b00f-766248d7f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79053f4-316f-4d2d-81bd-e6e04cfa81bf",
   "metadata": {},
   "source": [
    "## Load Model Checkpoint\n",
    "First, we will load the pre-trained checkpoint using the `load_pretrained()` function. You can specify the path to a checkpoint directory or a HuggingFace path.\n",
    "\n",
    "Below, we are loading directly from HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04953-869d-48a8-a2df-e601324e97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossformer.model.crossformer_model import CrossFormerModel\n",
    "\n",
    "model = CrossFormerModel.load_pretrained(\"hf://rail-berkeley/crossformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Next, we will load a trajectory from the Bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['CURL_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(\n",
    "    builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\"\n",
    ")\n",
    "ds = builder.as_dataset(split=\"train[:1]\")\n",
    "\n",
    "# sample episode and resize to 224x224 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode[\"steps\"])\n",
    "images = [\n",
    "    cv2.resize(np.array(step[\"observation\"][\"image\"]), (224, 224)) for step in steps\n",
    "]\n",
    "\n",
    "# extract goal image and language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = (\n",
    "    steps[0][\"observation\"][\"natural_language_instruction\"].numpy().decode()\n",
    ")\n",
    "\n",
    "# visualize episode\n",
    "print(f\"Instruction: {language_instruction}\")\n",
    "#mediapy.show_video(images, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cc324-0fcf-48b7-9468-324926035c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad64434",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "\n",
    "# create task dictionary\n",
    "task = model.create_tasks(\n",
    "    goals={\"image_primary\": goal_image[None]}\n",
    ")  # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])  # for language conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference loop, the model only uses 3rd person image observations for bridge\n",
    "\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step : step + WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        \"image_primary\": input_images,\n",
    "        \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "    }\n",
    "\n",
    "    # we need to pass in the dataset statistics to unnormalize the actions\n",
    "    actions = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        head_name=\"single_arm\",\n",
    "        unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "    )\n",
    "    actions = actions[0]  # remove batch\n",
    "\n",
    "    pred_actions.append(actions)\n",
    "    final_window_step = step + WINDOW_SIZE - 1\n",
    "    true_actions.append(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                steps[final_window_step][\"action\"][\"world_vector\"],\n",
    "                steps[final_window_step][\"action\"][\"rotation_delta\"],\n",
    "                np.array(steps[final_window_step][\"action\"][\"open_gripper\"]).astype(\n",
    "                    np.float32\n",
    "                )[None],\n",
    "            ),\n",
    "            axis=-1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "## Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  # actions have batch, horizon, dim, in this example we just take the first action for simplicity\n",
    "  axs[action_label].plot(pred_actions[:, 0, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036884d-2352-48b2-8e46-00a1bd74f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step : step + WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        \"image_primary\": input_images,\n",
    "        \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "    }\n",
    "\n",
    "    # we need to pass in the dataset statistics to unnormalize the actions\n",
    "    rollout, token_types = analyze_attention(model, observation, task)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beb102-c2f7-459f-a7d5-f1e7e8737d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rollout.shape)\n",
    "print(rollout)\n",
    "print(rollout.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc0190-4787-46bc-aa6a-cb1bdcbd123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(visualization_utils)\n",
    "from crossformer.utils import visualization_utils\n",
    "fig = visualization_utils.plot_readout_attention(rollout, token_types, \"readout_single_arm\", observation, observation_type=\"_primary\", observation_image=observation[\"image_primary\"][0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
