{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534daf7f-4b6b-4357-9a38-9117f72ce9b4",
   "metadata": {},
   "source": [
    "# Step 1: Minimal CrossFormer Inference Example\n",
    "\n",
    "This Colab demonstrates how to load a pre-trained / finetuned CrossFormer checkpoint, run inference for a single-arm and bimanual manipulation system, and compare the outputs to the true actions.\n",
    "\n",
    "First, let's start with a minimal example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5312c8-fa0e-4f0e-91a6-0c4e82153f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from crossformer.model.crossformer_model import CrossFormerModel\n",
    "\n",
    "model = CrossFormerModel.load_pretrained(\"hf://rail-berkeley/crossformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2daa3a-e79a-4226-8d25-cb66fafd9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cells to add after the model loading section in inference_pretrained.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def get_attention_weights(model, observation, task):\n",
    "    \"\"\"Extract attention weights from the model's transformer.\"\"\"\n",
    "    # Run transformer with store_attention=True\n",
    "    transformer_outputs = model.module.apply(\n",
    "        {\"params\": model.params},\n",
    "        observation,\n",
    "        task,\n",
    "        observation[\"timestep_pad_mask\"],\n",
    "        train=False,\n",
    "        method=\"crossformer_transformer\",\n",
    "        mutable=['intermediates']  # This allows accessing intermediate values\n",
    "    )\n",
    "    # print(len(transformer_outputs))\n",
    "    # Get stored attention weights from all layers\n",
    "    if len(transformer_outputs) > 1:  # Check if we got variables dict back\n",
    "        _, variables = transformer_outputs\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Extract attention weights from each transformer block\n",
    "        for i in range(12):  # Assuming 12 layers as in the paper\n",
    "            block_name = f'encoderblock_{i}'\n",
    "            print(variables['intermediates']['crossformer_transformer']['BlockTransformer_0']['Transformer_0']['encoderblock_8']['MultiHeadDotProductAttention_0']['attention_weights'][0].shape)\n",
    "            if 'intermediates' in variables and block_name in variables['intermediates']['crossformer_transformer']['BlockTransformer_0']['Transformer_0']:\n",
    "                layer_attention = variables['intermediates']['crossformer_transformer']['BlockTransformer_0']['Transformer_0'][block_name]['MultiHeadDotProductAttention_0']['attention_weights'][0]\n",
    "                attention_weights.append(layer_attention)\n",
    "        \n",
    "        return attention_weights\n",
    "    return None\n",
    "\n",
    "def compute_rollout(attention_weights):\n",
    "    \"\"\"Compute attention rollout from attention weights.\"\"\"\n",
    "    # Average attention weights across heads\n",
    "    attention_weights = [np.mean(layer_weights, axis=1) for layer_weights in attention_weights]\n",
    "    \n",
    "    # Initialize rollout with first layer attention\n",
    "    rollout = attention_weights[0]\n",
    "    \n",
    "    # Propagate attention through layers\n",
    "    for attention in attention_weights[1:]:\n",
    "        rollout = np.matmul(attention, rollout)\n",
    "    \n",
    "    # Normalize rollout\n",
    "    rollout = rollout / rollout.sum(axis=-1, keepdims=True)\n",
    "    return rollout\n",
    "\n",
    "def visualize_attention(attention_rollout, images, save_path=None):\n",
    "    \"\"\"Visualize attention rollout overlaid on input images.\"\"\"\n",
    "    # Get number of timesteps\n",
    "    num_timesteps = len(images)\n",
    "    \n",
    "    # Create a figure with subplots for each timestep\n",
    "    fig, axes = plt.subplots(2, num_timesteps, figsize=(4*num_timesteps, 8))\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        print(t)\n",
    "        # Plot original image\n",
    "        # axes[0, t].imshow(images[t])\n",
    "        # axes[0, t].axis('off')\n",
    "        # axes[0, t].set_title(f'Timestep {t}')\n",
    "        axes[t].imshow(images[t])\n",
    "        axes[t].axis('off')\n",
    "        axes[t].set_title(f'Timestep {t}')\n",
    "\n",
    "        # Get attention weights for this timestep\n",
    "        attention_map = attention_rollout[t].mean(axis=0)  # Average over batch dimension\n",
    "        attention_map = attention_map.reshape(-1)  # Flatten attention map\n",
    "        \n",
    "        # Create attention heatmap overlay\n",
    "        attention_resized = attention_map.reshape(14, 14)  # Assuming 14x14 attention grid\n",
    "        attention_resized = cv2.resize(attention_resized, (224, 224))  # Resize to image size\n",
    "        \n",
    "        # Plot attention heatmap\n",
    "        im = axes[1, t].imshow(attention_resized, cmap='hot', alpha=0.7)\n",
    "        axes[1, t].axis('off')\n",
    "        axes[1, t].set_title('Attention Map')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes.ravel().tolist())\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage cell:\n",
    "# Get attention weights for our example sequence\n",
    "# attention_weights = get_attention_weights(model, observation, task)\n",
    "\n",
    "# if attention_weights:\n",
    "#     # Compute attention rollout\n",
    "#     rollout = compute_rollout(attention_weights)\n",
    "    \n",
    "#     # Visualize attention for each timestep\n",
    "#     visualize_attention(rollout, images)\n",
    "# else:\n",
    "#     print(\"No attention weights were captured. Make sure the model is configured to store attention weights.\")\n",
    "\n",
    "# Modified original inference cell to include attention visualization\n",
    "def run_inference_with_attention(model, images, goal_image=None, language_instruction=None):\n",
    "    \"\"\"Run inference and visualize attention patterns.\"\"\"\n",
    "    # Create task dictionary\n",
    "    if goal_image is not None:\n",
    "        task = model.create_tasks(goals={\"image_primary\": goal_image[None]})\n",
    "    elif language_instruction is not None:\n",
    "        task = model.create_tasks(texts=[language_instruction])\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either goal image or language instruction\")\n",
    "\n",
    "    # Stack images into observation\n",
    "    input_images = np.stack(images)[None]  # Add batch dimension\n",
    "    observation = {\n",
    "        \"image_primary\": input_images,\n",
    "        \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "    }\n",
    "\n",
    "    # Get attention weights and actions\n",
    "    attention_weights = get_attention_weights(model, observation, task)\n",
    "    actions = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        head_name=\"single_arm\",\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "    )\n",
    "    \n",
    "    # Compute and visualize attention rollout\n",
    "    if attention_weights:\n",
    "        rollout = compute_rollout(attention_weights)\n",
    "        visualize_attention(rollout, images)\n",
    "    \n",
    "    return actions, attention_weights\n",
    "\n",
    "# Example usage:\n",
    "# actions, attention_weights = run_inference_with_attention(\n",
    "#     model,\n",
    "#     images,\n",
    "#     language_instruction=\"pick up the spoon\"\n",
    "# )\n",
    "\n",
    "# You can also analyze specific attention patterns\n",
    "def analyze_attention_patterns(attention_weights, layer_idx=None):\n",
    "    \"\"\"Analyze attention patterns in specific layers or across all layers.\"\"\"\n",
    "    if layer_idx is not None:\n",
    "        # Analyze specific layer\n",
    "        layer_attention = attention_weights[layer_idx]\n",
    "        avg_attention = np.mean(layer_attention, axis=(0,1))  # Average over batch and heads\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(avg_attention, cmap='viridis')\n",
    "        plt.title(f'Average Attention Pattern - Layer {layer_idx}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Analyze attention across all layers\n",
    "        num_layers = len(attention_weights)\n",
    "        fig, axes = plt.subplots(2, num_layers//2, figsize=(20,8))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            avg_attention = np.mean(attention_weights[i], axis=(0,1))\n",
    "            sns.heatmap(avg_attention, cmap='viridis', ax=axes[i])\n",
    "            axes[i].set_title(f'Layer {i}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# # Example: Analyze attention patterns\n",
    "# if attention_weights:\n",
    "#     # Analyze a specific layer\n",
    "#     analyze_attention_patterns(attention_weights, layer_idx=0)\n",
    "    \n",
    "#     # Analyze all layers\n",
    "#     analyze_attention_patterns(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05934a4-ff4c-4aac-8d85-d8e700568d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll demonstrate how to create an observation and task dictionary for a bimanual task. \n",
    "# Then we'll use them to sample an action from the model.\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "# create a random image\n",
    "img = np.random.randint(0, 255, size=(224, 224, 3))\n",
    "# add batch and observation history dimension (CrossFormer accepts a history of up to 5 time-steps)\n",
    "img = img[None, None]\n",
    "# our bimanual training data has an overhead view and two wrist views\n",
    "observation = {\n",
    "    \"image_high\": img,\n",
    "    \"image_left_wrist\": img,\n",
    "    \"image_right_wrist\": img,\n",
    "    \"timestep_pad_mask\": np.array([[True]]),\n",
    "}\n",
    "# create a task dictionary for a language task\n",
    "task = model.create_tasks(texts=[\"uncap the pen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc750a90-e9fc-448e-97f8-21d0ca3b93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights separately\n",
    "attention_weights = get_attention_weights(model, observation, task)\n",
    "if attention_weights:\n",
    "    # Compute rollout\n",
    "    rollout = compute_rollout(attention_weights)\n",
    "    \n",
    "    # Visualize attention maps\n",
    "    visualize_attention(rollout, [img.squeeze()])\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    analyze_attention_patterns(attention_weights, layer_idx=0)  # Look at first layer\n",
    "    analyze_attention_patterns(attention_weights)  # Look at all layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e90cb-ac85-46c0-8ea0-aa8e96d0cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rollout.shape)\n",
    "print(model.__dir__())\n",
    "print(type(model.config))\n",
    "print(model.config.keys())\n",
    "print(model.config[\"model\"].keys())\n",
    "print(model.config[\"model\"][\"token_embedding_size\"])\n",
    "print(type(model.module))\n",
    "print(model.module.__dir__())\n",
    "#print(model.module)\n",
    "\n",
    "#for i in range(len(attention_weights)):\n",
    "#    print(attention_weights[i].shape)\n",
    "#visualize_attention(rollout, [img.squeeze()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b803d89-3bc3-4613-96a5-142445f2e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout, token_map = model.analyze_attention(observation, task, head_name=\"single_arm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85932127-fc94-4cb6-873f-517b6133e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_readout_attention(\n",
    "    model,\n",
    "    observation,\n",
    "    task,\n",
    "    readout_name=\"readout_single_arm\",  # or \"bimanual\" for bimanual actions\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d262e2-d08d-470d-a7ba-bf1456e31060",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_attention_rollout(\n",
    "    model,\n",
    "    observation,\n",
    "    task,\n",
    "    readout_name=\"readout_single_arm\"  # or \"readout_bimanual\"\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ad189-0402-4dae-a6fa-3ff3dce28db2",
   "metadata": {},
   "source": [
    "# Step 2: Run Inference on Full Trajectories\n",
    "\n",
    "That was easy! Now let's try to run inference across a whole single-arm trajectory and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51eb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mediapy for visualization\n",
    "!pip install mediapy\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f7fd1-5b43-480f-b00f-766248d7f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79053f4-316f-4d2d-81bd-e6e04cfa81bf",
   "metadata": {},
   "source": [
    "## Load Model Checkpoint\n",
    "First, we will load the pre-trained checkpoint using the `load_pretrained()` function. You can specify the path to a checkpoint directory or a HuggingFace path.\n",
    "\n",
    "Below, we are loading directly from HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04953-869d-48a8-a2df-e601324e97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossformer.model.crossformer_model import CrossFormerModel\n",
    "\n",
    "model = CrossFormerModel.load_pretrained(\"hf://rail-berkeley/crossformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Next, we will load a trajectory from the Bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['CURL_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(\n",
    "    builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\"\n",
    ")\n",
    "ds = builder.as_dataset(split=\"train[:1]\")\n",
    "\n",
    "# sample episode and resize to 224x224 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode[\"steps\"])\n",
    "images = [\n",
    "    cv2.resize(np.array(step[\"observation\"][\"image\"]), (224, 224)) for step in steps\n",
    "]\n",
    "\n",
    "# extract goal image and language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = (\n",
    "    steps[0][\"observation\"][\"natural_language_instruction\"].numpy().decode()\n",
    ")\n",
    "\n",
    "# visualize episode\n",
    "print(f\"Instruction: {language_instruction}\")\n",
    "mediapy.show_video(images, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad64434",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "# create task dictionary\n",
    "task = model.create_tasks(\n",
    "    goals={\"image_primary\": goal_image[None]}\n",
    ")  # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])  # for language conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference loop, the model only uses 3rd person image observations for bridge\n",
    "\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step : step + WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        \"image_primary\": input_images,\n",
    "        \"timestep_pad_mask\": np.full((1, input_images.shape[1]), True, dtype=bool),\n",
    "    }\n",
    "\n",
    "    # we need to pass in the dataset statistics to unnormalize the actions\n",
    "    actions = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        head_name=\"single_arm\",\n",
    "        unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "    )\n",
    "    actions = actions[0]  # remove batch\n",
    "\n",
    "    pred_actions.append(actions)\n",
    "    final_window_step = step + WINDOW_SIZE - 1\n",
    "    true_actions.append(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                steps[final_window_step][\"action\"][\"world_vector\"],\n",
    "                steps[final_window_step][\"action\"][\"rotation_delta\"],\n",
    "                np.array(steps[final_window_step][\"action\"][\"open_gripper\"]).astype(\n",
    "                    np.float32\n",
    "                )[None],\n",
    "            ),\n",
    "            axis=-1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "## Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  # actions have batch, horizon, dim, in this example we just take the first action for simplicity\n",
    "  axs[action_label].plot(pred_actions[:, 0, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4136b9-0a50-4565-9005-1c13435b5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "def plot_full_evaluation(model, images, pred_actions, true_actions, tasks, steps_to_show=None):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive visualization showing images, attention, and action predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: CrossFormer model instance\n",
    "        images: List of images from the episode\n",
    "        pred_actions: Predicted actions array\n",
    "        true_actions: Ground truth actions array\n",
    "        tasks: Task dict for attention computation\n",
    "        steps_to_show: Optional list of timesteps to show (default: every 3rd step)\n",
    "    \"\"\"\n",
    "    if steps_to_show is None:\n",
    "        steps_to_show = range(0, len(images), 3)\n",
    "    \n",
    "    # Build image strip\n",
    "    img_strip = np.concatenate([images[i] for i in steps_to_show], axis=1)\n",
    "    \n",
    "    # Action dimension labels\n",
    "    ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "    \n",
    "    # Set up the figure layout - now including attention maps\n",
    "    figure_layout = [\n",
    "        ['image'] * len(ACTION_DIM_LABELS),\n",
    "        ['attention'] * len(ACTION_DIM_LABELS),\n",
    "        ACTION_DIM_LABELS\n",
    "    ]\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, axs = plt.subplot_mosaic(figure_layout, figsize=(45, 15))\n",
    "    \n",
    "    # Plot image strip\n",
    "    axs['image'].imshow(img_strip)\n",
    "    axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "    axs['image'].set_title('Episode Images')\n",
    "    \n",
    "    # For each timestep, create observation and get attention\n",
    "    attention_maps = []\n",
    "    for t in steps_to_show:\n",
    "        # Create observation for this timestep with correct window size\n",
    "        window_size = 5  # default window size, adjust if needed\n",
    "        start_t = max(0, t - window_size + 1)\n",
    "        obs_images = images[start_t:t+1]\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(obs_images) < window_size:\n",
    "            padding = [obs_images[0]] * (window_size - len(obs_images))\n",
    "            obs_images = padding + obs_images\n",
    "        \n",
    "        observation = {\n",
    "            \"image_primary\": np.stack(obs_images)[None],  # Add batch dim\n",
    "            \"timestep_pad_mask\": np.ones((1, window_size), dtype=bool)\n",
    "        }\n",
    "        \n",
    "        # Get transformer outputs\n",
    "        transformer_outputs = model.run_transformer(\n",
    "            observation, tasks, observation[\"timestep_pad_mask\"], train=False\n",
    "        )\n",
    "        \n",
    "        # Compute attention\n",
    "        primary_tokens = transformer_outputs['obs'].tokens[0, -1]\n",
    "        d_k = primary_tokens.shape[-1]\n",
    "        attention_weights = jnp.einsum('nd,md->nm', primary_tokens, primary_tokens) / jnp.sqrt(d_k)\n",
    "        attention_weights = jax.nn.softmax(attention_weights, axis=-1)\n",
    "        \n",
    "        # Reshape attention to grid\n",
    "        n_tokens = attention_weights.shape[0]\n",
    "        grid_size = int(np.sqrt(n_tokens))\n",
    "        attention_map = attention_weights[:grid_size*grid_size, :grid_size*grid_size]\n",
    "        attention_map = attention_map.reshape(grid_size, grid_size)\n",
    "        attention_maps.append(attention_map)\n",
    "    \n",
    "    # Plot attention strip\n",
    "    attention_strip = np.concatenate(attention_maps, axis=1)\n",
    "    axs['attention'].imshow(attention_strip, cmap='viridis')\n",
    "    axs['attention'].set_xlabel('Time in one episode (subsampled)')\n",
    "    axs['attention'].set_title('Attention Maps')\n",
    "    \n",
    "    # Plot actions\n",
    "    for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "        axs[action_label].plot(pred_actions[:, 0, action_dim], label='predicted action')\n",
    "        axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "        axs[action_label].set_title(action_label)\n",
    "        axs[action_label].set_xlabel('Time in one episode')\n",
    "    \n",
    "    plt.legend()\n",
    "    return fig\n",
    "\n",
    "# Example usage in the evaluation section:\n",
    "\"\"\"\n",
    "# Run evaluation with attention visualization\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "\n",
    "# Create task dict\n",
    "task = model.create_tasks(texts=[language_instruction])\n",
    "\n",
    "# Create visualization\n",
    "fig = plot_full_evaluation(\n",
    "    model,\n",
    "    images,\n",
    "    pred_actions,\n",
    "    true_actions,\n",
    "    task\n",
    ")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a58e41-2bf9-49d3-82c5-d9390b432999",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_readout_attention(\n",
    "    model,\n",
    "    observation,\n",
    "    task,\n",
    "    readout_name=\"readout_single_arm\"\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041d545-781f-4291-b9b5-5e37153f3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_attention_rollout(\n",
    "    model,\n",
    "    observation,\n",
    "    task,\n",
    "    readout_name=\"readout_single_arm\"  # or \"readout_bimanual\"\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036884d-2352-48b2-8e46-00a1bd74f19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
